# mapping_test_v5.py (å„ªåŒ–å¾Œçš„æ¸¬è©¦ç¨‹å¼)
# ==========================================
# TinyLlama mapping å¾®èª¿æ¨¡å‹æ¸¬è©¦ï¼ˆå„ªåŒ–æ ¼å¼ V5ï¼‰
# ==========================================
import json
import random
import csv
import re
from tqdm import tqdm
from unsloth import FastLanguageModel
import torch
import os
# ------------------------------------------
# 1ï¸âƒ£ åŸºæœ¬è¨­å®š
# ------------------------------------------
N_SAMPLES = 50  # æ¸¬è©¦æ¨£æœ¬æ•¸
TEMP_JSON_PATH = "mapping_test_v5.jsonl"
OUTPUT_CSV_PATH = "mapping_test_v5_result.csv"
WRONG_CSV_PATH = "mapping_test_v5_result_wrong.csv"
MODEL_PATH = "./TinyLlama-finetune-mapping"  # ä½ è¨“ç·´å¥½çš„æ¨¡å‹è³‡æ–™å¤¾
NOTFOUND_RATIO = 0.15
# çµ±ä¸€ç‚ºè¨“ç·´æ™‚è¨­å®šçš„æ¨™æº–è² é¢å›æ‡‰
NEGATIVE_RESPONSE_STANDARD = "no such coordinates" 
COORD_REGEX = re.compile(r"\(\s*(-?\d+)\s*,\s*(-?\d+)\s*\)")
LOCATIONS = [
    "kitchen", "bedroom", "bathroom", "living room", "garage", "office",
    "garden", "balcony", "dining room", "hallway", "attic", "basement",
    "rooftop", "study room", "storage", "laundry room", "guest room",
    "playroom", "conference room", "server room", "gym", "library",
    "theater", "parking lot", "workshop"
]
QUERY_TEMPLATES = [
    "where is {place}?",
    "what is the location of {place}?",
    "coordinates of {place}?",
    "give me the coordinates of {place}.",
    "find {place}.",
    "tell me the coordinates for {place}."
]
FORMATS = [
    "{loc}: ({x},{y})",
    "{loc} at ({x}, {y})",
    "{loc} = {x},{y}",
    "{loc}: [{x}, {y}]",
    "{loc} â†’ ({x},{y})",
]
# ------------------------------------------
# 2ï¸âƒ£ ç”Ÿæˆæ¸¬è©¦è³‡æ–™ï¼ˆèˆ‡è¨“ç·´è³‡æ–™ç”Ÿæˆé‚è¼¯ V2 ä¸€è‡´ï¼‰
# ------------------------------------------
def generate_mapping_test_dataset(n_samples=N_SAMPLES, filename=TEMP_JSON_PATH):
    with open(filename, "w", encoding="utf-8") as f:
        for _ in tqdm(range(n_samples), desc="Generating mapping test dataset (v5 format)"):
            num_locs = random.randint(3, 8)
            chosen_locs = random.sample(LOCATIONS, k=num_locs)
            coords = {loc: (random.randint(-50, 50), random.randint(-50, 50)) for loc in chosen_locs}
            # 80% æ©Ÿç‡å«è² è™Ÿ
            for loc in coords:
                if random.random() < 0.8:
                    x_sign = random.choice([-1, 1])
                    y_sign = random.choice([-1, 1])
                    x, y = coords[loc]
                    coords[loc] = (x * x_sign, y * y_sign)
            # 15% æ©Ÿç‡å•ä¸åˆ°çš„åœ°é»
            ask_notfound = random.random() < NOTFOUND_RATIO
            if ask_notfound:
                available = [loc for loc in LOCATIONS if loc not in chosen_locs]
                query_loc = random.choice(available)
            else:
                query_loc = random.choice(chosen_locs)
            query_sentence = random.choice(QUERY_TEMPLATES).format(place=query_loc)
            # æ‰“äº‚ mapping é †åº
            random.shuffle(chosen_locs)
            coord_map = {loc: coords[loc] for loc in chosen_locs}
            # å¤šæ¨£åŒ–æ ¼å¼
            lines = []
            for loc in chosen_locs:
                fmt = random.choice(FORMATS)
                x, y = coord_map[loc]
                lines.append(fmt.format(loc=loc, x=x, y=y))
            mapping_text = "\n".join(lines)
            input_text = f"{mapping_text}\n\nQuestion: {query_sentence}"
            
            # ğŸ’¡ çµ±ä¸€è¼¸å‡º "no such coordinates"
            if ask_notfound:
                output_text = NEGATIVE_RESPONSE_STANDARD 
            else:
                x, y = coord_map[query_loc]
                output_text = f"({x},{y})"
                
            json.dump({"input": input_text, "output": output_text}, f, ensure_ascii=False)
            f.write("\n")
    print(f"âœ… å·²ç”Ÿæˆ {n_samples} ç­†æ¸¬è©¦è³‡æ–™è‡³ {filename}")
# ------------------------------------------
# 3ï¸âƒ£ è¼‰å…¥æ¨¡å‹
# ------------------------------------------
print(f"ğŸš€ è¼‰å…¥æ¨¡å‹ä¸­ï¼š{MODEL_PATH} ...")
# è¼‰å…¥é‚è¼¯ä¸è®Š
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_PATH,
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)
print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
device = "cuda" if torch.cuda.is_available() else "cpu"
try:
    model.to(device)
except Exception:
    pass
# ------------------------------------------
# 4ï¸âƒ£ è©•ä¼°å‡½å¼ (é—œéµå„ªåŒ–)
# ------------------------------------------
def extract_first_coordinate(text: str):
    # æå–ç¬¬ä¸€å€‹åº§æ¨™
    if text is None:
        return None
    m = COORD_REGEX.search(text)
    if not m:
        return None
    x, y = m.group(1), m.group(2)
    return f"({x},{y})"

def evaluate_model_on_dataset(dataset_path=TEMP_JSON_PATH, output_csv=OUTPUT_CSV_PATH, wrong_csv=WRONG_CSV_PATH):
    with open(dataset_path, "r", encoding="utf-8") as f:
        data_list = [json.loads(line) for line in f]
    results = []
    correct = 0
    for data in tqdm(data_list, desc="Evaluating model"):
        user_prompt = data["input"]
        # çµ±ä¸€æ¸…ç†æ­£ç¢ºç­”æ¡ˆ
        correct_output = data["output"].strip().lower().replace(" ", "").replace("'","").replace('"',"") 
        
        system_prompt = (
            "Extract coordinates. "
            "Output ONLY (x,y) if found. "
            "Output ONLY 'no such coordinates' if not found. "
            "No extra text."
        )
        
        # ğŸš¨ é—œéµå„ªåŒ– 1: ä½¿ç”¨èˆ‡è¨“ç·´ä¸€è‡´çš„ Prompt æ ¼å¼
        # é€™è£¡ä½¿ç”¨ train.py ä¸­å»ºè­°çš„ Llama/Mistral/TinyLlama æ¨™æº–æŒ‡ä»¤æ ¼å¼
        # prompt = (
        #     f"<s>[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{user_prompt} [/INST] "
        # )

        prompt = (
            f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
            f"<|im_start|>user\n{user_prompt}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        input_length = inputs["input_ids"].shape[1] # å–å¾—è¼¸å…¥é•·åº¦

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,
                temperature=0.1,
                eos_token_id=[
                    tokenizer.eos_token_id, # ç¢ºä¿ç”¨ tokenizer.eos_token_id (å¯èƒ½ç‚º </s>)
                    tokenizer.convert_tokens_to_ids("[/INST]"), # æœ‰æ™‚æ¨¡å‹æœƒé‡è¤‡ [INST]
                ],
                pad_token_id=tokenizer.pad_token_id,
            )
        
        # ğŸš¨ é—œéµå„ªåŒ– 2: åƒ…è§£ç¢¼æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†
        generated_tokens = outputs[0][input_length:]
        decoded = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip().lower()
        
        # ğŸš¨ é—œéµå„ªåŒ– 3: åš´æ ¼æ¯”å°è¼¸å‡º
        pred_coord = extract_first_coordinate(decoded)
        
        if pred_coord:
            pred_clean = pred_coord.replace(" ", "")
        elif NEGATIVE_RESPONSE_STANDARD in decoded:
            pred_clean = NEGATIVE_RESPONSE_STANDARD
        # çµ±ä¸€è¼¸å‡ºå¾Œï¼Œä¸å†éœ€è¦æ¯”å° "location not found" å’Œ "unknown place"
        else:
            # å¦‚æœæ¨¡å‹è¼¸å‡ºæ—¢éåº§æ¨™ä¹Ÿéæ¨™æº–éŒ¯èª¤è¨Šæ¯ï¼Œå‰‡è¦–ç‚ºéŒ¯èª¤ï¼Œä¸¦è¨˜éŒ„åŸå§‹è¼¸å‡º
            pred_clean = decoded.split('\n')[0].strip() 
            
        is_correct = (pred_clean == correct_output)
        
        if is_correct:
            correct += 1
        results.append({
            "input": prompt,
            "expected": correct_output,
            "predicted_raw": decoded,
            "predicted_clean": pred_clean,
            "correct": is_correct,
        })
        
    total = len(results)
    accuracy = correct / total * 100 if total > 0 else 0.0
    print(f"\nğŸ“Š å…± {total} ç­†ï¼Œæ­£ç¢º {correct} ç­†ï¼Œæº–ç¢ºç‡ = {accuracy:.2f}%")
    # è¼¸å‡ºçµæœ
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["input", "expected", "predicted_raw", "predicted_clean", "correct"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for r in results:
            writer.writerow(r)
    print(f"âœ… å·²è¼¸å‡ºå®Œæ•´çµæœè‡³ {output_csv}")
    wrongs = [r for r in results if not r["correct"]]
    if wrongs:
        with open(wrong_csv, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for r in wrongs:
                writer.writerow(r)
        print(f"âŒ éŒ¯èª¤æ¨£æœ¬å·²è¼¸å‡ºè‡³ {wrong_csv}ï¼Œå…± {len(wrongs)} ç­†ã€‚")
    else:
        print("ğŸ‰ ç„¡éŒ¯èª¤æ¨£æœ¬ã€‚")
    return accuracy, results
# ------------------------------------------
# 5ï¸âƒ£ ä¸»æµç¨‹
# ------------------------------------------
if __name__ == "__main__":
    generate_mapping_test_dataset()
    evaluate_model_on_dataset()