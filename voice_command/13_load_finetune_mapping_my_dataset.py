# mapping_test_v3.py
# ==========================================
# Mapping è‡ªå‹•ç”Ÿæˆ + TinyLlama æ¨¡å‹æ¸¬è©¦ + ç²¾ç¢ºåº§æ¨™æ¯”å° + åŒ¯å‡º CSV (å«éŒ¯èª¤æ¨£æœ¬)
# ==========================================

import json
import random
import csv
import re
from tqdm import tqdm
from unsloth import FastLanguageModel
import torch
import os

# ------------------------------------------
# 1ï¸âƒ£ åƒæ•¸è¨­å®š
# ------------------------------------------

N_SAMPLES = 30  # æ¸¬è©¦æ¨£æœ¬æ•¸
TEMP_JSON_PATH = "mapping_test_100.jsonl"
OUTPUT_CSV_PATH = "mapping_test_result.csv"
WRONG_CSV_PATH = "mapping_test_result_wrong.csv"
MODEL_PATH = "./TinyLlama-finetune-mapping"  # ä½ è¨“ç·´å®Œçš„æ¨¡å‹è³‡æ–™å¤¾

LOCATIONS = [
    "kitchen", "bedroom", "bathroom", "living room", "garage", "office",
    "garden", "balcony", "dining room", "hallway", "attic", "basement",
    "rooftop", "study room", "storage", "laundry room", "guest room",
    "playroom", "conference room", "server room", "gym", "library",
    "theater", "parking lot", "workshop"
]

QUERY_TEMPLATES = [
    "where is {place}?",
    "what is the location of {place}?",
    "coordinates of {place}?"
]

COORD_REGEX = re.compile(r"\(\s*(-?\d+)\s*,\s*(-?\d+)\s*\)")

# ------------------------------------------
# 2ï¸âƒ£ ç”Ÿæˆæ¸¬è©¦æ•¸æ“šé›† (åŒè¨“ç·´æ ¼å¼)
# ------------------------------------------

def generate_mapping_test_dataset(n_samples=N_SAMPLES, filename=TEMP_JSON_PATH):
    with open(filename, "w", encoding="utf-8") as f:
        for _ in tqdm(range(n_samples), desc="Generating mapping test dataset (v2 format)"):
            num_locs = random.randint(3, 10)
            chosen_locs = random.sample(LOCATIONS, k=num_locs)
            coords = {loc: (random.randint(-50, 50), random.randint(-50, 50)) for loc in chosen_locs}

            # ç´„ 70% è² åº§æ¨™æ©Ÿç‡
            for loc in coords:
                if random.random() < 0.7:
                    coords[loc] = (coords[loc][0] * random.choice([-1, 1]),
                                   coords[loc][1] * random.choice([-1, 1]))

            query_idx = random.choices(range(num_locs), weights=[0.05] + [0.15] * (num_locs - 1))[0]
            query_loc = chosen_locs[query_idx]
            query_sentence = random.choice(QUERY_TEMPLATES).format(place=query_loc)

            random.shuffle(chosen_locs)
            coord_map = {loc: coords[loc] for loc in chosen_locs}

            input_lines = []
            for loc in chosen_locs:
                x, y = coord_map[loc]
                input_lines.append(f"{loc}: ({x},{y})")
            input_lines.append(f"QUERY: {query_sentence}")
            # æ›´å£èªåŒ–çš„æ ¼å¼ç´„æŸï¼ˆè·Ÿè¨“ç·´æ™‚ä¸€è‡´ï¼‰
            input_lines.append("ANSWER FORMAT: (x,y)")

            input_text = "\n".join(input_lines)
            output_text = f"({coord_map[query_loc][0]},{coord_map[query_loc][1]})"

            json.dump({"input": input_text, "output": output_text}, f, ensure_ascii=False)
            f.write("\n")

    print(f"âœ… å·²ç”Ÿæˆ {n_samples} ç­†æ¸¬è©¦è³‡æ–™è‡³ {filename}")


# ------------------------------------------
# 3ï¸âƒ£ è¼‰å…¥æ¨¡å‹
# ------------------------------------------

print(f"ğŸš€ è¼‰å…¥æ¨¡å‹ä¸­ï¼š{MODEL_PATH} ...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_PATH,
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)
print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")

# æŠŠ model æ”¾åˆ° cudaï¼ˆè‹¥æ”¯æ´ï¼‰
device = "cuda" if torch.cuda.is_available() else "cpu"
try:
    model.to(device)
except Exception:
    pass  # FastLanguageModel å¯èƒ½å·²å…§éƒ¨è™•ç† device

# ------------------------------------------
# å·¥å…·å‡½å¼ï¼šç²¾ç¢ºæŠ½åº§æ¨™
# ------------------------------------------

def extract_first_coordinate(text: str):
    """
    å¾æ–‡å­—ä¸­æŠ½å‡ºç¬¬ä¸€å€‹ (x,y)ï¼Œå›å‚³åƒ '(x,y)' çš„å­—ä¸²ï¼Œæˆ– Noneã€‚
    """
    if text is None:
        return None
    m = COORD_REGEX.search(text)
    if not m:
        return None
    x, y = m.group(1), m.group(2)
    return f"({x},{y})"

# ------------------------------------------
# 4ï¸âƒ£ è‡ªå‹•æ¸¬è©¦ + ç²¾ç¢ºæ¯”å° + åŒ¯å‡º CSVï¼ˆå«éŒ¯èª¤æ¨£æœ¬ï¼‰
# ------------------------------------------

def evaluate_model_on_dataset(dataset_path=TEMP_JSON_PATH, output_csv=OUTPUT_CSV_PATH, wrong_csv=WRONG_CSV_PATH):
    results = []
    with open(dataset_path, "r", encoding="utf-8") as f:
        data_list = [json.loads(line) for line in f]

    total = len(data_list)
    correct = 0

    for data in tqdm(data_list, desc="Evaluating model"):
        user_prompt = data["input"]
        correct_output = data["output"].strip().replace(" ", "")

        # èˆ‡è¨“ç·´ä¸€è‡´çš„ prompt
        prompt = f"""<|im_start|>system
You are a mapping assistant. Always answer only with coordinates in (x,y) format.<|im_end|>
<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""

        inputs = tokenizer(prompt, return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,  # deterministic for mapping
                temperature=0.1,
                top_k=30,
                top_p=0.9,
                eos_token_id=[
                    tokenizer.convert_tokens_to_ids("<|im_end|>"),
                    tokenizer.convert_tokens_to_ids("<|end_of_conversation|>"),
                ],
                pad_token_id=tokenizer.pad_token_id,
            )

        # decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        predicted_extracted = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # å–å¾— assistant å›ç­”æ®µè½
        # if "<|im_start|>assistant" in decoded:
        #     decoded = decoded.split("<|im_start|>assistant")[-1].strip()
        # # ç§»é™¤å¯èƒ½çš„çµå°¾ token èˆ‡å¾ŒçºŒæ–‡å­—
        # decoded = decoded.split("<|end_of_conversation|>")[0].strip()

        # æŠ½å‡ºç¬¬ä¸€çµ„ (x,y)
        # predicted_extracted = extract_first_coordinate(decoded)
        is_correct = (predicted_extracted is not None) and (predicted_extracted.replace(" ", "") == correct_output)

        if is_correct:
            correct += 1

        results.append({
            "input": user_prompt,
            "expected": correct_output,
            # "predicted_raw": decoded,
            "predicted_extracted": predicted_extracted,
            "correct": is_correct
        })

    accuracy = correct / total * 100 if total > 0 else 0.0
    print(f"\nğŸ“Š ç¸½å…± {total} ç­†ï¼Œæ­£ç¢º {correct} ç­†ï¼Œæº–ç¢ºç‡ = {accuracy:.2f}%")

    # åŒ¯å‡ºå…¨éƒ¨çµæœ CSV
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["input", "expected", "predicted_raw", "predicted_extracted", "correct"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for r in results:
            writer.writerow(r)
    print(f"âœ… å…¨éƒ¨çµæœå·²è¼¸å‡ºåˆ° {output_csv}")

    # åŒ¯å‡ºéŒ¯èª¤æ¨£æœ¬ CSVï¼ˆæ–¹ä¾¿äººå·¥æª¢æŸ¥ï¼‰
    wrongs = [r for r in results if not r["correct"]]
    if wrongs:
        with open(wrong_csv, "w", newline="", encoding="utf-8") as csvfile:
            fieldnames = ["input", "expected", "predicted_raw", "predicted_extracted", "correct"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for r in wrongs:
                writer.writerow(r)
        print(f"âŒ éŒ¯èª¤æ¨£æœ¬å·²è¼¸å‡ºåˆ° {wrong_csv} ï¼ˆå…± {len(wrongs)} ç­†ï¼‰")
    else:
        print("ğŸ‰ ç„¡éŒ¯èª¤æ¨£æœ¬ã€‚")

    return accuracy, results

# ------------------------------------------
# 5ï¸âƒ£ ä¸»æµç¨‹
# ------------------------------------------

if __name__ == "__main__":
    # 1) ç”Ÿæˆæ¸¬è©¦é›†
    generate_mapping_test_dataset()

    # 2) åŸ·è¡Œè©•ä¼°ä¸¦è¼¸å‡º CSV
    evaluate_model_on_dataset()
